# Databricks notebook source
# MAGIC %md
# MAGIC # Analyze Unity Catalog Blob Sizes
# MAGIC 
# MAGIC This notebook takes the CSV output from the `analyze_uc_blob_sizes.py` script (which lists individual blob paths and sizes) and aggregates the sizes per Unity Catalog table UUID.
# MAGIC 
# MAGIC **Prerequisite:** You must first upload the `sorted_subfolders_final.csv` file generated by the Python script to a Databricks table using the UI (**+ New** -> **Add data** -> **Upload file**).

# COMMAND ----------

# MAGIC %md
# MAGIC ## Imports

# COMMAND ----------

from pyspark.sql import functions as F

# COMMAND ----------

# MAGIC %md
# MAGIC ## Configuration

# COMMAND ----------

# ===============================================
# === USER CONFIGURATION ========================
# ===============================================
# !! Replace with the actual catalog, schema, and table name you created during UI upload !!
TABLE_FULL_NAME = "your_catalog.your_schema.uploaded_blob_sizes"
# ===============================================
# === END USER CONFIGURATION ====================
# ===============================================

# COMMAND ----------

# MAGIC %md
# MAGIC ## Read Uploaded Data

# COMMAND ----------

print(f"Reading data from table: {TABLE_FULL_NAME}")
try:
    df = spark.read.table(TABLE_FULL_NAME)
    # Optional: Display raw data
    # display(df.limit(10))
except Exception as e:
    print(f"Error reading table {TABLE_FULL_NAME}. Please check the name and ensure it exists.")
    raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## Aggregate Table Sizes
# MAGIC 
# MAGIC Extract the table UUID from the blob path (`Subfolder` column) and sum the `Size` for each UUID.

# COMMAND ----------

print("Aggregating storage size by table UUID...")
# The Subfolder path looks like: metastore/<metastore_uuid>/tables/<table_uuid>/<optional_partition_dirs>/<filename>
# We split the string by '/' and get the 4th element which should be the table UUID.
df_agg = (
    df.withColumn("path_parts", F.split(F.col("Subfolder"), "/"))
    # Use element_at(4) because F.split creates an array and Spark SQL's array indexing is 1-based.
    .withColumn("table_uuid", F.element_at(F.col("path_parts"), 4))
    # Ensure Size is numeric for summation
    .withColumn("Size", F.col("Size").cast("long"))
    .groupBy("table_uuid")
    .agg(F.sum("Size").alias("total_size_bytes"))
    # Add human-readable sizes
    .withColumn("total_size_gb", F.round(F.col("total_size_bytes") / (1024**3), 2))
    .withColumn("total_size_tb", F.round(F.col("total_size_bytes") / (1024**4), 4))
    .orderBy(F.col("total_size_bytes").desc())
)

print("Aggregation complete.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Display Aggregated Results

# COMMAND ----------

print(f"\nAggregated sizes per table UUID from {TABLE_FULL_NAME}:")
results_to_display = df_agg.select(
    "table_uuid",
    "total_size_bytes",
    "total_size_gb",
    "total_size_tb"
)
display(results_to_display) # Use display() for richer table view in Databricks

# COMMAND ----------

# MAGIC %md
# MAGIC ## Visualization Suggestion
# MAGIC 
# MAGIC Use the Databricks visualization tools on the table output above to create a bar chart showing table sizes by UUID.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Optional: Join with Information Schema
# MAGIC 
# MAGIC This section attempts to join the aggregated sizes with the `system.information_schema.tables` to get human-readable table names corresponding to the UUIDs.
# MAGIC 
# MAGIC **Note:** Requires appropriate privileges on system tables (SELECT on `system.information_schema.tables`). The system catalog must also be enabled and shared with your workspace.

# COMMAND ----------

print("\nAttempting to join with information_schema.tables to get table names...")
try:
    # Adjust schema name if your system catalog uses a different one
    tables_info = spark.read.table("system.information_schema.tables")

    # Extract UUID from table_properties['StorageLocation'] which might look like:
    # '.../metastore-uuid/tables/table-uuid'
    # Or sometimes from table_url if available and formatted suitably.
    # Let's try extracting from StorageLocation as it's more standard for managed tables.
    # Note: This regex assumes the UUID is the last part of the path after '/tables/'.
    tables_info = tables_info.withColumn(
        "extracted_uuid",
        F.regexp_extract(F.col("storage_path"), r'/tables/([0-9a-fA-F\-]+)$', 1)
    )
    # Filter out rows where UUID extraction failed (e.g., external tables with different paths)
    tables_info_filtered = tables_info.filter(F.col("extracted_uuid") != "")

    joined_df = df_agg.join(
        tables_info_filtered,
        df_agg["table_uuid"] == tables_info_filtered["extracted_uuid"],
        "left" # Use left join to keep all aggregated sizes, even if name isn't found
    ).select(
        tables_info_filtered["table_catalog"],
        tables_info_filtered["table_schema"],
        tables_info_filtered["table_name"],
        df_agg["table_uuid"],
        df_agg["total_size_bytes"],
        df_agg["total_size_gb"],
        df_agg["total_size_tb"]
    ).orderBy(F.col("total_size_bytes").desc())

    print("\nJoined results with table names (showing top results):")
    display(joined_df) # Use display() for richer table view
except Exception as e:
    print(f"\nCould not join with information_schema.tables. This is optional.")
    print(f"  Error: {e}")
    print("  Ensure you have privileges and the system catalog is enabled and shared.")

# COMMAND ----------

print("\nNotebook finished.") 